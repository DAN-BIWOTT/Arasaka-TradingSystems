{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DAN-BIWOTT/Arasaka-TradingSystems/blob/main/Ensembled_SVR_%26_SFN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YE4wy6KRfJMZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2a2dd097-60fd-470b-ef96-1ae069684991"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting skl2onnx\n",
            "  Downloading skl2onnx-1.18.0-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.21.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting onnx>=1.2.1 (from skl2onnx)\n",
            "  Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: scikit-learn>=1.1 in /usr/local/lib/python3.11/dist-packages (from skl2onnx) (1.6.1)\n",
            "Collecting onnxconverter-common>=1.7.0 (from skl2onnx)\n",
            "  Downloading onnxconverter_common-1.14.0-py2.py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (25.2.10)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (24.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (5.29.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.13.1)\n",
            "Collecting protobuf (from onnxruntime)\n",
            "  Downloading protobuf-3.20.2-py2.py3-none-any.whl.metadata (720 bytes)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1->skl2onnx) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1->skl2onnx) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1->skl2onnx) (3.6.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Downloading skl2onnx-1.18.0-py2.py3-none-any.whl (300 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.3/300.3 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.21.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m112.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m121.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxconverter_common-1.14.0-py2.py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-3.20.2-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf, humanfriendly, onnx, coloredlogs, onnxruntime, onnxconverter-common, skl2onnx\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.4\n",
            "    Uninstalling protobuf-5.29.4:\n",
            "      Successfully uninstalled protobuf-5.29.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 3.20.2 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.2 which is incompatible.\n",
            "tensorflow-metadata 1.17.1 requires protobuf<6.0.0,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.2 which is incompatible.\n",
            "tensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 3.20.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnx-1.17.0 onnxconverter-common-1.14.0 onnxruntime-1.21.1 protobuf-3.20.2 skl2onnx-1.18.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "65c7635fac1c4afba6a3c4d40f25479d"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install skl2onnx onnxruntime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-Z_Z0MjZSNy"
      },
      "source": [
        "# Import Data Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xm-LVm23YkBz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rKdlCChKZXeB"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive, files\n",
        "drive.mount('/content/drive') # mount your google colab file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "Swa5ksDEZZiQ",
        "outputId": "5e9c88ee-b59d-4680-c834-d598f1846d49"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Market Data GBPUSD before 2025.02.28'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-659df45f061b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfilename_in_drive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Market Data GBPUSD before 2025.02.28\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_in_drive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# Version Number\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_version\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"2.1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Extract date from filename_in_drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Market Data GBPUSD before 2025.02.28'"
          ]
        }
      ],
      "source": [
        "filename_in_drive = \"/content/drive/My Drive/Forex/Market Data GBPUSD before 2025.02.28.csv\"\n",
        "data = pd.read_csv(filename_in_drive)\n",
        "# Version Number\n",
        "model_version = \"2.1\"\n",
        "# Extract date from filename_in_drive\n",
        "match = re.search(r'\\b(\\d{4}\\.\\d{2}\\.\\d{2})\\b', filename_in_drive)\n",
        "if match:\n",
        "    extracted_date = match.group(1)\n",
        "else:\n",
        "    print(\"No date found in the filename.\")\n",
        "data.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQ7dwLCUZiSQ"
      },
      "outputs": [],
      "source": [
        "num_rows = len(data)\n",
        "num_rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZ2kGh1pZkQ4"
      },
      "outputs": [],
      "source": [
        "data = data[::-1]\n",
        "# Reduce the dataset to the first 10,000 rows because\n",
        "data = data.head(10000)\n",
        "original_data = data.copy()\n",
        "\n",
        "data.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmQgddKYZnWJ"
      },
      "outputs": [],
      "source": [
        "num_rows = len(data)\n",
        "num_rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YPo7kW4Zpb4"
      },
      "outputs": [],
      "source": [
        "data.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwnLaCU8ZqGY"
      },
      "outputs": [],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1ANiDOicDB3"
      },
      "outputs": [],
      "source": [
        "#Define the forecast horizon\n",
        "look_ahead = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zglecGg1cFDX"
      },
      "outputs": [],
      "source": [
        "#Calculate the median, which is the midpoint between the high and low prices.\n",
        "data['Candlestick_Median'] = (data['High'] + data['Low']) / 2\n",
        "\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRSJREiTcJDe"
      },
      "outputs": [],
      "source": [
        "data['Candlestick_Median'] = data['Candlestick_Median'].shift(-1)\n",
        "data = data.rename(columns={'Candlestick_Median': 'Future_Median'})\n",
        "data['Price_Difference'] = data['High'] - data['Low']\n",
        "data['Open_Close_Change_Pct'] = (data['Close'] - data['Open']) / data['Open'] * 100\n",
        "data['High_Low_Change_Pct'] = (data['High'] - data['Low']) / data['Low'] * 100\n",
        "data['Volume_MA_20'] = data['Volume'].rolling(window=20).mean() # 20-period moving average of volume\n",
        "data['Volume_Change_Pct'] = data['Volume'].pct_change() * 100 # Percentage change in volume"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "46SAwnLfQM_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vg6z9tD2u6eW"
      },
      "outputs": [],
      "source": [
        "# Create lagged median prices\n",
        "data['median_t-1'] = data['Future_Median'].shift(2)\n",
        "data['median_t-2'] = data['Future_Median'].shift(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGKHSjRgcLNm"
      },
      "outputs": [],
      "source": [
        "# Update features (X)\n",
        "X = data[[\"Open\", \"High\", \"Low\", \"Close\", \"Price_Difference\", \"Open_Close_Change_Pct\",\n",
        "          \"High_Low_Change_Pct\", \"Volume\", \"Volume_MA_20\", \"Volume_Change_Pct\",\n",
        "          \"median_t-1\", \"median_t-2\"]].copy()\n",
        "y = data[\"Future_Median\"]  # Target variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqufPcuXcNi_"
      },
      "outputs": [],
      "source": [
        "# Removing NAN Values while preserving time series structure\n",
        "# Creating a temporary DataFrame combining X and y for consistent indexing\n",
        "temp_df = pd.concat([X, y], axis=1)\n",
        "\n",
        "# Dropping rows with NaN values in any column (features or target)\n",
        "temp_df.dropna(inplace=True)\n",
        "\n",
        "# Extractting updated X and y from the temporary DataFrame\n",
        "X = temp_df[X.columns]  # Features\n",
        "y = temp_df[y.name]  # Target variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1trvhic5ZsUA"
      },
      "source": [
        " Ensemble an SVR and an SFN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMpF_sufZ46o"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import LinearSVR\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.ensemble import VotingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Szyk5x3EL-q3"
      },
      "source": [
        "scale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kB8vwIbnZ_8H"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iY95J65ELr1v"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"NaN values in X (features):\")\n",
        "print(X[X.isnull().any(axis=1)])  # Display rows with NaN in X\n",
        "\n",
        "print(\"\\nNaN values in y (target):\")\n",
        "print(y[y.isnull()])"
      ],
      "metadata": {
        "id": "W9iJuL-yU0-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oufziy6CamJf"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {'C': [0.1, 1, 10], 'epsilon': [0.01, 0.1, 1]}  # Example hyperparameter grid\n",
        "grid_search = GridSearchCV(LinearSVR(max_iter=10000), param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "best_linear_svr_model = grid_search.best_estimator_"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SFN with Grid search"
      ],
      "metadata": {
        "id": "JC3nBz7dWbW4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sJzIpEVdC92"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "# Define the hyperparameter grid for the SFN (MLPRegressor)\n",
        "param_grid_sfn = {\n",
        "    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],\n",
        "    'activation': ['relu', 'tanh'],\n",
        "    'solver': ['adam'],\n",
        "    'alpha': [0.0001, 0.001, 0.01],\n",
        "    'learning_rate': ['constant', 'adaptive']\n",
        "}\n",
        "\n",
        "# Create GridSearchCV object for the SFN\n",
        "grid_search_sfn = GridSearchCV(MLPRegressor(max_iter=1000, random_state=42),\n",
        "                               param_grid_sfn, cv=5, scoring='neg_mean_squared_error')\n",
        "\n",
        "# Fit the GridSearchCV object to the training data\n",
        "grid_search_sfn.fit(X_train, y_train)\n",
        "\n",
        "# Get the best SFN model\n",
        "best_sfn_model = grid_search_sfn.best_estimator_\n",
        "\n",
        "print(\"Best SFN Model:\")\n",
        "print(best_sfn_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Temporary SFN without Grid search"
      ],
      "metadata": {
        "id": "a8hchxG3WU4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "# # Define the SFN model with pre-selected hyperparameters\n",
        "# best_sfn_model = MLPRegressor(\n",
        "#     hidden_layer_sizes=(100,),  # Example: Using one hidden layer with 100 neurons\n",
        "#     activation='relu',        # Example: Using ReLU activation function\n",
        "#     solver='adam',            # Example: Using the Adam optimizer\n",
        "#     alpha=0.001,             # Example: Setting regularization strength\n",
        "#     learning_rate='constant', # Example: Using a constant learning rate\n",
        "#     max_iter=1000,           # Maximum number of iterations\n",
        "#     random_state=42          # For reproducibility\n",
        "# )\n",
        "\n",
        "# # Fit the SFN model to the training data\n",
        "# best_sfn_model.fit(X_train, y_train)\n",
        "\n",
        "# print(\"SFN Model:\")\n",
        "# print(best_sfn_model)"
      ],
      "metadata": {
        "id": "QoIV51D-WQEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VotyyAejarZ3"
      },
      "outputs": [],
      "source": [
        "ensemble_model = VotingRegressor(estimators=[('linear_svr', best_linear_svr_model), ('sfn', best_sfn_model)],\n",
        "                                 weights=[0.7, 0.3])  # Example: 0.7 for LinearSVR, 0.3 for SFN\n",
        "ensemble_model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCYKe-7Lasc4"
      },
      "outputs": [],
      "source": [
        "# Evaluate the ensemble model\n",
        "y_pred = ensemble_model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error of Ensemble: {mse}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbRVdXAibDpo"
      },
      "source": [
        "Graph the perfomance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UULbxwzbGJ_"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# 1. Predictions (Same as before)\n",
        "y_train_pred = ensemble_model.predict(X_train)\n",
        "y_test_pred = ensemble_model.predict(X_test)\n",
        "\n",
        "# 2. Training Data Performance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(y_train.index, y_train, label='Actual Median', color='blue')  # Actual median in blue\n",
        "plt.plot(y_train.index, y_train_pred, label='Predicted Median', color='red', linestyle='--')  # Predicted median in dashed red\n",
        "plt.title('Training Data: Actual vs. Predicted Median Price')\n",
        "plt.xlabel('Data Point Index')\n",
        "plt.ylabel('Median Price')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 3. Testing Data Performance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(y_test.index, y_test, label='Actual Median', color='blue')  # Actual median in blue\n",
        "plt.plot(y_test.index, y_test_pred, label='Predicted Median', color='red', linestyle='--')  # Predicted median in dashed red\n",
        "plt.title('Testing Data: Actual vs. Predicted Median Price')\n",
        "plt.xlabel('Data Point Index')\n",
        "plt.ylabel('Median Price')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xswi0VayeO7E"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score\n",
        "\n",
        "# 1. Calculate Performance Metrics\n",
        "train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "test_mse = mean_squared_error(y_test, y_test_pred)\n",
        "\n",
        "train_rmse = np.sqrt(train_mse)\n",
        "test_rmse = np.sqrt(test_mse)\n",
        "\n",
        "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
        "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
        "\n",
        "train_r2 = r2_score(y_train, y_train_pred)\n",
        "test_r2 = r2_score(y_test, y_test_pred)\n",
        "\n",
        "train_explained_variance = explained_variance_score(y_train, y_train_pred)\n",
        "test_explained_variance = explained_variance_score(y_test, y_test_pred)\n",
        "\n",
        "# 2. Store Metrics in a Dictionary\n",
        "performance_metrics = {\n",
        "    \"train_mse\": train_mse,\n",
        "    \"test_mse\": test_mse,\n",
        "    \"train_rmse\": train_rmse,\n",
        "    \"test_rmse\": test_rmse,\n",
        "    \"train_mae\": train_mae,\n",
        "    \"test_mae\": test_mae,\n",
        "    \"train_r2\": train_r2,\n",
        "    \"test_r2\": test_r2,\n",
        "    \"train_explained_variance\": train_explained_variance,\n",
        "    \"test_explained_variance\": test_explained_variance\n",
        "}\n",
        "\n",
        "# 3. Get Features and Model Name\n",
        "features = list(X.columns)\n",
        "model_name = f\"{model_version}_Ensemble Model (Linear SVR + SFN)_{extracted_date}\"  # Update model name\n",
        "\n",
        "# 4. (Optional) Scaling Parameters - If you used scaling, include this part\n",
        "scaling_parameters = {\n",
        "    \"mean\": scaler.mean_.tolist(),\n",
        "    \"std_dev\": scaler.scale_.tolist()\n",
        "}\n",
        "\n",
        "# 5. Create Log Data Dictionary\n",
        "log_data = {\n",
        "    \"model_name\": model_name,\n",
        "    \"features\": features,\n",
        "    \"performance_metrics\": performance_metrics,\n",
        "    # \"scaling_parameters\": scaling_parameters  # Include if you have scaling parameters\n",
        "}\n",
        "\n",
        "# 6. Save to JSON File\n",
        "# Assuming 'extracted_date' is defined earlier in your code\n",
        "with open(f\"{model_name}_log_{extracted_date}.json\", \"w\") as f:\n",
        "    json.dump(log_data, f, indent=4)\n",
        "\n",
        "print(f\"{model_version}_Model log saved to {model_name}_log_{extracted_date}.json\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Scaling Mean:\", ', '.join(map(str, scaler.mean_)))\n",
        "print(\"Scaling Standard Deviation:\", ', '.join(map(str, scaler.scale_)))"
      ],
      "metadata": {
        "id": "GPKoKgu66YbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_Q02LakgwCE"
      },
      "source": [
        "ONNX file Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PyHcL79g0hs"
      },
      "outputs": [],
      "source": [
        "from skl2onnx import convert_sklearn\n",
        "from skl2onnx.common.data_types import FloatTensorType\n",
        "import onnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4aFSfc3g3QU"
      },
      "outputs": [],
      "source": [
        "initial_type = [('input', FloatTensorType([None, X_test.shape[1]]))]  # Define input shape based on your features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AtVT6FWg6qj"
      },
      "outputs": [],
      "source": [
        "# Convert weights to a NumPy array before converting to ONNX\n",
        "ensemble_model.weights = np.array(ensemble_model.weights)\n",
        "\n",
        "initial_type = [('input', FloatTensorType([None, X_test.shape[1]]))]  # Define input shape based on your features\n",
        "onnx_model = convert_sklearn(ensemble_model, initial_types=initial_type)\n",
        "with open(f\"{model_version}_ensemble_model_{extracted_date}.onnx\", \"wb\") as f:\n",
        "    f.write(onnx_model.SerializeToString())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}